\section{Introduction}

Many engineering and scientific applications often require the solution of
linear systems
 \begin{equation}
 Ax = b, \qquad x,b \in \Bbb{R}^n,\, A\in \Bbb{R}^{n\times n},
\label{eq:linsys}
\end{equation}
where the matrix $A$ is large and sparse. In our context, \emph{large}
means millions or even billions of unknowns, whereas
\emph{sparse} means that most coefficients in $A$ are zero and it is
convenient to employ special storage formats. In many applications,
and specifically in the test cases discussed in this paper, the matrix
$A$ is also symmetric and positive definite. 

The solution of such systems is achieved by using either direct or iterative methods. 
However, for large and sparse linear systems, direct methods are  difficult to implement in parallel
because of their recursive nature and often suffer from high computational complexity~\cite{Templates}.  
Therefore, a more viable alternative is to use iterative solvers and Krylov
subspace methods are the most widely used class of these solvers~\cite{MR1990645}. 
%The most widely used solvers for systems of this type are Krylov subspace methods~\cite{MR1990645}.
Their time to solution is determined by the number
of iterations performed and by the time per iteration, and hence their 
efficiency is the result of a tradeoff between iteration complexity and speed of convergence. 
A critical feature is the application of preconditioning,
corresponding, e.g., to a transformation of the form
\begin{equation}\label{eq:preclinsys}
M^{-1}Ax=M^{-1}b,
\end{equation} 
which is aimed at speeding up the convergence of Krylov methods through the improvement of
the ``quality'' of the system matrix. Note that the
product $M^{-1}A$ is not computed explicitly, but the Krylov solvers
are modified so that it is obtained by performing at each iteration
the computation of $M^{-1} v$, where $v$ is a suitable vector. 

Multigrid methods are widely used as preconditioners, because they are optimal,
in the sense that their computational cost grows linearly with the number of unknowns
when the linear systems come from the discretization of elliptic partial differential
equations~\cite{Vassilevski2008}.
Many efforts have been performed to extend their efficiency
to more general systems, often the direction of a complete
algebraic approach, where no information on the origin of the problem
is exploited. In this case, they are called Algebraic MultiGrid (AMG) methods~\cite{Stuben2001}.

The linear complexity of AMG preconditioners generally
translates into algorithmic scalability, i.e., the number of iterations
of AMG-preconditioned Krylov solvers does not depend on the size of the
problem. This allows efficient parallel implementations on multiple CPUs,
for example through a domain decomposition approach, in which rows of
the matrix are assigned to different computing nodes.
The diffusion of General Purpose Graphics Processing Units (GPGPUs),
currently found in many of the fastest supercomputers on the Top 500 list
(\url{https://www.top500.org/lists/}), requires the exposition of a high
degree of parallelism, because GPUs are high-throughput many-core processors.
Since AMG methods are obtained by combining different components (smoother,
coarsening algorithm, coarsest-level solver, restriction and prolongation operators),
a full exploitation of GPU capabilities requires each component to be optimized
for this type of architecture.

In this work, we focus on the choice and implementation of AMG smoothers
and coarsest-level solvers capable of harnessing the computational power offered by
a cluster of GPUs. We consider block-Jacobi smoothers and solvers that use sparse
approximate inverses to perform the local solves required by each block, instead of typical
incomplete factorization methods. The choice of sparse approximate inverses
is motivated by the much larger amount of parallelism exposed by
sparse matrix-vector products as compared to the parallelism available
in sparse triangular solves. Furthermore, suitable implementations of sparse approximate
inverse preconditioners have already proved their efficiency on a single GPU (see, e.g.,
\cite{BERTACCINI2016693,Lukash:2012}). 
In our work, the smoothers and local solvers are used within
the AMG framework offered by the MLD2P4 package of preconditioners~\cite{mld-toms,Dambra:2007} 
and exploiting sparse matrix data structures and Krylov solvers from
the PSBLAS  library~\cite{PSBLAS3}. 
We conducted an extensive set of weak scalability tests on the JURECA supercomputer at the J\"ulich Supercomputing Centre (JSC), 
using a groundwater modelling application developed at JSC. 
To quantify the gain in efficiency and performance using the cluster of GPUs, we also ran tests only on CPUs. 
Our results show scalability up to 128 GPUs and 256 millions equations.

%and are analyzed in terms of both execution speed and algorithmic scalability. 
Some research efforts have been devoted to study the computation of preconditions on GPUs, e.g.,~\cite{Dehnavi:2013}, 
showing the acceleration that can be achieved by exploiting GPUs. 
%
To the best of our knowledge, the selected AMG smoothers and local solvers we employ in they work 
have not been considered in other AMG preconditioners tailored for GPUs~\cite{BellDaltonOlson:2012}. 
%\textbf{Aggiungere biblio:
%Bell, Dalton, Olson; Park, Smelianskiy, Yang, ...; Rupp, Weinbub}. 
Furthermore, most results have been so far presented on a single GPU; 
for example, Richter et al.~\cite{Richter:2014} presented the implementation for discrete elliptic field problems 
using the CUSP library. 

In this work, we do not consider the setup phase of the preconditioner,
which will be the subject of future work. 
Of course, an efficient AMG setup on multiple GPUs is important for the overall efficiency of the
preconditioner. Nevertheless, in situations where we need to solve multiple
linear systems with the same coefficient matrix, it is desirable to obtain
the best possible efficiency during the solution phase even at the expense of a
longer setup time, because this will be amortized over multiple
solution steps.

The remainder of this article is organized as follows. In Section~\ref{AMG} we briefly describe AMG preconditioners,
identifying their main sparse matrix kernels. In Section~\ref{SLA-GPU}, we discuss the main issues
concerning the implementation of these kernels on GPUs. In Section~\ref{results} we illustrate the
behaviour of the AMG preconditioners on a cluster of GPUs on linear systems coming from a
a groundwater modelling applications. 
We conclude in Section~\ref{sec:concl} with some final remarks.  